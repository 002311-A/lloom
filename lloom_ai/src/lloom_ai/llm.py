"""
llm.py
------
This file contains utility functions for processing calls to LLMs.
"""

# IMPORTS ================================
import openai
import google.generativeai as palm

import time
import random
from pathos.multiprocessing import Pool
import hashlib

# from langchain.llms import OpenAI
import asyncio
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# CONSTANTS ================================
SYS_TEMPLATE = "You are a helpful assistant who helps with identifying patterns in text examples."

RATE_LIMITS = {
    # https://platform.openai.com/account/limits
    # (n_requests, wait_time_secs)
    "gpt-3.5-turbo": (300, 10), # = 300*6 = 1800 rpm (max 10k requests per minute for org)
    "gpt-4": (10, 10), # = 10*6 = 60 rpm (max 10k requests per minute for org)
    "gpt-4-turbo-preview": (10, 10), # = 10*6 = 60 rpm, testing purposes
    "palm": (9, 10), # = 9*6 = 54 rpm (max 90 requests per minute)
}

def get_system_prompt():
    system_message_prompt = SystemMessagePromptTemplate.from_template(SYS_TEMPLATE)
    return system_message_prompt

# RETRYING + MULTIPROCESSING ================================

""" 
Defines a Python decorator to avoid API rate limits by retrying with exponential backoff.
From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb
"""
def retry_with_exponential_backoff(
    func,
    initial_delay: float = 1,
    exponential_base: float = 2,
    jitter: bool = True,
    max_retries: int = 3,
    errors: tuple = (openai.error.RateLimitError,),
):
    """Retry a function with exponential backoff."""

    def wrapper(*args, **kwargs):
        # Initialize variables
        num_retries = 0
        delay = initial_delay

        # Loop until a successful response or max_retries is hit or an exception is raised
        while True:
            try:
                return func(*args, **kwargs)

            # Retry on specified errors
            except errors as e:
                # Increment retries
                num_retries += 1

                # Check if max retries has been reached
                if num_retries > max_retries:
                    raise Exception(
                        f"Maximum number of retries ({max_retries}) exceeded."
                    )

                # Increment the delay
                delay *= exponential_base * (1 + jitter * random.random())

                # Sleep for the delay
                print("DELAY ", delay)
                time.sleep(delay)

            # Raise exceptions for any errors not specified
            except Exception as e:
                print("Exception", e)

    return wrapper

# CUSTOM LLM API WRAPPERS ================================
def get_res_str(res):
    # Fetch the response string from an LLM response JSON
    return res["choices"][0]["message"]["content"]

@retry_with_exponential_backoff
def base_api_wrapper(cur_prompt, model_name="gpt-3.5-turbo"):
    # Wrapper for calling the base OpenAI API
    res = openai.ChatCompletion.create(
        model=model_name,
        messages=[
                {"role": "system", "content": SYS_TEMPLATE},
                {"role": "user", "content": cur_prompt},
            ]
    )
    res_str = get_res_str(res)
    return res_str

@retry_with_exponential_backoff
def base_api_wrapper_palm(cur_prompt):
    # Uses the chat model (chat-bison-001)
    response = palm.chat(messages=cur_prompt)
    res_str = response.last
    return res_str

""" 
Handles multiprocessing of multiple model queries

Inputs: 
    - prompts: list of prompts to execute
    - multiprocessing: bool. Run multiprocessing if True, otherwise run sequentially

Outputs: List of outputs generated by all models
"""
def multi_query(prompts, model_name, multiprocessing=True):
    if multiprocessing: 
        if model_name != "palm":
            # GPT
            pool = Pool(processes=len(prompts))
            results = pool.map(base_api_wrapper, prompts) # unpacks prompt and model name
            pool.close()
        elif model_name == "palm":
            # PaLM
            batch_size = 90
            delay = 60
            # Run only `batch_size` prompts at a time
            for i in range(0, len(prompts), batch_size):
                cur_prompts = prompts[i:i+batch_size]
                pool = Pool(processes=len(cur_prompts))
                results = pool.map(base_api_wrapper_palm, prompts) # unpacks prompt and model name
                pool.close()
                time.sleep(delay)

    else:
        results = []
        for p, model_name in prompts:
            r = base_api_wrapper(p, model_name)
            results.append(r)
    
    return results

# Mock classes for testing ====================================
class LangChainGeneration():
    # Mock class for a generation within a LangChainResult
    def __init__(self, response):
        self.text = response

class LangChainResult:
    # Mock class for a result from a LangChain API call
    def __init__(self, response):
        gen = LangChainGeneration(response)
        self.generations = [[gen]]
        self.llm_output = {
            "token_usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
            }
        }

async def get_test_response(example_ids):
    # Returns a dummy response for testing, currently only for the `themes_to_examples` prompt
    # TODO: Adapt to support other prompts
    full_template = """
    {{
        "pattern_results": [
            {results_str}
        ]
    }}
    """
    def get_ex_json(ex_id):
        template = """
            {{
                "example_id": "{example_id}",
                "rationale": "sample rationale",
                "answer": "A",
            }}
        """
        return template.format(example_id=ex_id)
    
    results = [get_ex_json(ex_id) for ex_id in example_ids]
    results_str = ",\n".join(results)
    res = LangChainResult(response=full_template.format(results_str=results_str))
    return res

def get_prompt_hash(p):
    user_message = p[1].content  # Isolate the user message
    hash = hashlib.sha256(user_message.encode()).hexdigest()
    return hash

# Main function making calls to LLM
async def multi_query_gpt(chat_model, prompt_template, arg_dict, batch_num=None, wait_time=None, cache=False, debug=False):
    # Run a single query using LangChain OpenAI Chat API

    # System
    system_message_prompt = get_system_prompt()
    # Human
    human_message_prompt = HumanMessagePromptTemplate.from_template(prompt_template)
    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
    
    # Execute prompt
    if wait_time is not None:
        if debug:
            print(f"Batch {batch_num}, wait time {wait_time}")
        await asyncio.sleep(wait_time)  # wait asynchronously
    chat_prompt_formatted = chat_prompt.format_prompt(**arg_dict).to_messages()
    prompt_hash = get_prompt_hash(chat_prompt_formatted)

    # Run LLM generation
    res_full = await chat_model.agenerate([chat_prompt_formatted])
    
    # TODO: Add back caching
    return res_full

def process_results(results):
    # Extract just the text generation from the LangChain OpenAI Chat results
    res_text = [res.generations[0][0].text for res in results]
    return res_text

async def multi_query_gpt_wrapper(prompt_template, arg_dicts, model_name, temperature=0, batch_num=None, batched=True, debug=False):
    # Multi-query using LangChain OpenAI Chat API
    chat_model = ChatOpenAI(temperature=temperature, model_name=model_name)
    if debug:
        print("model_name", model_name)
    

    if not batched:
        # Non-batched version
        tasks = [multi_query_gpt(chat_model, prompt_template, args) for args in arg_dicts]
    else:
        # Batched version
        n_requests, wait_time_secs = RATE_LIMITS[model_name]
        tasks = []
        arg_dict_batches = [arg_dicts[i:i + n_requests] for i in range(0, len(arg_dicts), n_requests)]
        for inner_batch_num, cur_arg_dicts in enumerate(arg_dict_batches):
            chat_model = ChatOpenAI(temperature=temperature, model_name=model_name)
            if batch_num is None:
                wait_time = wait_time_secs * inner_batch_num
            else:
                wait_time = wait_time_secs * batch_num
            if debug:
                wait_time = 0 # Debug mode
            cur_tasks = [multi_query_gpt(chat_model, prompt_template, arg_dict=args, batch_num=batch_num, wait_time=wait_time) for args in cur_arg_dicts]
            tasks.extend(cur_tasks)

    res_full = await asyncio.gather(*tasks)

    res_text = process_results(res_full)
    return res_text, res_full


# Currently unused
# def call_wrapper(sess, prompt_template, arg_dict, model_name=None, verbose=False):
#     # Wrapper for calling an LLM API, whether the OpenAI API (via base_api_wrapper) or other APIs like LangChain
#     res = None
#     if model_name is None:
#         model_name = sess.model
#     if sess.use_base_api:
#         if arg_dict is None:
#             cur_prompt = prompt_template
#         else:
#             cur_prompt = prompt_template.format(**arg_dict)
#         if verbose:
#             print(cur_prompt)
        
#         if model_name != "palm":
#             # GPT
#             resp = base_api_wrapper(cur_prompt, model_name)
#         elif model_name == "palm":
#             # PaLM
#             resp = base_api_wrapper_palm(cur_prompt)
#         res = resp
#     else:
#         raise Exception("Not implemented yet!")
    
#     # Save raw response to session's llm_cache
#     t = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())
#     sess.llm_cache[t] = res
#     return res
